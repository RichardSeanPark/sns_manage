# AI 및 LLM 최신 뉴스 자동 수집·요약 시스템 개발 계획서

## 개발 개요

이 문서는 AI 및 LLM 최신 뉴스 자동 수집·요약 시스템을 1인 개발자가 1-2주 내에 목업을 완성하고, 총 1달 이내에 전체 개발을 마무리하기 위한 구체적인 개발 계획을 담고 있습니다. 

### 시스템 핵심 기능 요약
- **수집**: RSS 피드, 웹 크롤링, API를 통한 AI 관련 뉴스 자동 수집
- **선별**: 키워드 필터링, 출처 신뢰도, 트렌드 등을 기준으로 콘텐츠 선별
- **생성**: LLM 기반 요약 및 분석 자동 생성
- **게시**: 사용자 검토/수정 후 네이버 카페 등에 선택적 게시

### 핵심 아키텍처 접근법
- **OpenAI Agent 기반 구현**: 모든 기능을 OpenAI Agent를 활용하여 에이전트화
- **MCP 서버 아키텍처**: 각 모듈을 독립적인 MCP(Model Context Protocol) 서버로 구현하여 유연성과 확장성 확보

## 1. 개발 전략 및 우선순위

### 핵심 원칙
1. **단순화 우선**: 복잡한 기능보다 핵심 기능을 먼저 구현하고 점진적으로 개선
2. **모듈식 설계**: 각 기능을 독립적 모듈로 구현하여 개발 및 테스트 용이성 확보
3. **빠른 프로토타이핑**: 실제 데이터로 작동하는 최소 시스템을 빠르게 구축
4. **지속적 반복**: 기능 단위로 개발-테스트-개선 주기 반복
5. **에이전트 중심 설계**: 각 기능을 OpenAI Agent 기반 에이전트로 구현하여 지능적 자동화 실현
6. **MCP 서버 표준화**: 표준 MCP 프로토콜을 통해 모든 모듈 간 일관성 있는 상호작용 보장

### 개발 우선순위
1. 뉴스 수집 모듈 (RSS 피드 중심)
2. 기본 선별 로직 (키워드 필터링 우선)
3. LLM 요약 모듈 (GPT 연동)
4. 웹 기반 사용자 인터페이스
5. 네이버 카페 게시 모듈
6. OpenAI Agent 통합 및 MCP 서버 구현
7. 추가 기능 및 최적화

## 2. 목업 개발 계획 (1-2주)

### 1주차: 핵심 기능 구현 및 데이터 파이프라인 설정

#### 1일차: 개발 환경 및 프로젝트 구조 설정
- 개발 환경 설정 (Python 3.10+, 필요 라이브러리 설치)
- 프로젝트 구조 설계 및 GitHub 저장소 생성
- 기본 설정 파일 및 로깅 시스템 구성

#### 2일차: RSS 피드 수집 모듈 개발
- `feedparser` 라이브러리를 활용한 RSS 수집 기능 구현
- 주요 AI 관련 RSS 피드 소스 5-10개 설정
- 수집된 데이터 임시 저장소 구현 (JSON 파일 기반)

#### 3일차: 기본 선별 로직 개발
- 키워드 필터링 기능 구현 (기본 AI 관련 키워드 리스트)
- 간단한 중복 검사 기능 구현 (URL 기반)
- 필터링된 뉴스 JSON 포맷 정의 및 저장

#### 4일차: LLM 요약 모듈 구현
- OpenAI API 연동 (GPT 모델 활용)
- 요약 및 분석을 위한 프롬프트 템플릿 설계
- 뉴스 본문 추출 및 요약 프로세스 구현

#### 5일차: 기본 CLI 인터페이스 구현
- 수집-선별-요약 파이프라인 통합
- 콘솔 기반 결과 출력 및 상태 확인 기능
- 단위 테스트 작성 및 디버깅

#### 주말: 통합 테스트 및 보완
- 전체 파이프라인 테스트
- 발견된 문제점 수정
- 1주차 개발 결과 문서화

### 2주차: 사용자 인터페이스 및 게시 기능 구현

#### 1일차: 웹 프레임워크 설정
- Flask 또는 FastAPI 기반 웹 서버 설정
- 기본 라우팅 및 API 엔드포인트 구성
- 정적 파일 서빙 설정

#### 2일차: 기본 웹 UI 구현
- Bootstrap 또는 Tailwind CSS 활용한 기본 레이아웃
- 수집된 뉴스 목록 및 상세 페이지 구현
- 간단한 필터링 및 설정 인터페이스

#### 3일차: 사용자 검토 및 편집 기능
- 요약된 콘텐츠 미리보기 기능
- 편집 인터페이스 구현
- 승인/저장/삭제 기능 구현

#### 4일차: 네이버 카페 게시 모듈 개발
- 네이버 API 연동 및 인증 구현
- 게시 기능 구현 (제목, 본문, 해시태그 등)
- 게시 결과 확인 및 로깅

#### 5일차: 스케줄링 및 알림 기능
- 주기적 실행을 위한 스케줄러 구현 (APScheduler 활용)
- 이메일 또는 웹 알림 기능 구현
- 오류 처리 및 재시도 로직 구현

#### 주말: 통합 테스트 및 목업 완성
- 전체 시스템 통합 테스트
- UI/UX 개선
- 목업 버전 완성 및 문서화

## 3. 전체 개발 로드맵 (3-4주)

### 3주차: 기능 확장 및 안정화

#### 1-2일차: 추가 데이터 소스 연동
- 웹 크롤링 모듈 구현 (BeautifulSoup 또는 Scrapy 활용)
- 뉴스 API 연동 (구글 뉴스 API 등)
- 데이터 소스 관리 인터페이스 구현

#### 3-4일차: 고급 선별 로직 구현
- 벡터 임베딩 기반 유사도 검사 기능 구현
- 출처 신뢰도 평가 로직 구현
- 트렌드 감지 알고리즘 구현

#### 5-7일차: 데이터베이스 구현 및 마이그레이션
- PostgreSQL 또는 SQLite 데이터베이스 설계
- ORM 모델 구현 (SQLAlchemy)
- 기존 데이터 마이그레이션
- 관리자 페이지 개선

### 4주차: 최적화 및 마무리

#### 1-2일차: 성능 최적화
- 병목 지점 식별 및 해결
- 캐싱 전략 구현
- 비동기 처리 최적화

#### 3-4일차: UI/UX 개선
- 모바일 대응 UI 개선
- 사용자 설정 페이지 구현
- 통계 및 대시보드 구현

#### 5-6일차: 테스트 및 디버깅
- 전체 시스템 통합 테스트
- 경계 조건 및 예외 처리 테스트
- 버그 수정 및 안정화

#### 7일차: 배포 준비 및 문서화
- 배포용 Docker 컨테이너 구성
- 사용자 및 개발자 문서 작성
- 최종 빌드 및 테스트

## 4. 기술 스택 및 도구

### 백엔드
- **언어**: Python 3.10+
- **웹 프레임워크**: FastAPI
- **데이터베이스**: SQLite(목업), mariadb(최종)
- **ORM**: SQLAlchemy
- **스케줄링**: APScheduler
- **비동기 처리**: asyncio, aiohttp

### 프론트엔드
- **프레임워크**: React
- **CSS 프레임워크**: Tailwind CSS
- **빌드 도구**: Vite 또는 Webpack

### 인프라
- **버전 관리**: Git/GitHub
- **개발 환경**: Poetry 또는 Pipenv
- **배포**: Docker
- **CI/CD**: GitHub Actions

### 외부 API 및 서비스
- **LLM**: GEMMA3 27B API
- **게시 API**: 네이버 카페 API
- **뉴스 API**: Google News API, News API

## 5. 목업 개발의 범위와 한계

### 목업에 포함될 기능
- RSS 피드를 통한 뉴스 수집 (5-10개 소스)
- 기본 키워드 필터링
- GPT 기반 요약 생성
- 웹 기반 관리 인터페이스 (기본 기능)
- 네이버 카페 게시 기능 (기본)

### 목업에서 제외될 기능
- 고급 중복 감지 (임베딩 기반)
- 실시간 트렌드 분석
- 완전 자동화된 스케줄링
- 고급 사용자 설정
- 통계 및 분석 대시보드
- 대용량 처리 최적화

## 6. 개발 시작 가이드

### 환경 설정
```bash
# 프로젝트 폴더 생성
mkdir ai-news-system
cd ai-news-system

# 가상환경 설정
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 의존성 설치
pip install feedparser requests openai fastapi uvicorn sqlalchemy
pip install aiohttp jinja2 python-multipart

# 개발용 도구 설치
pip install pytest black isort
```

### 초기 프로젝트 구조
```
ai-news-system/
├── app/
│   ├── __init__.py
│   ├── main.py            # FastAPI 애플리케이션 진입점
│   ├── config.py          # 설정 관리
│   ├── collector/         # 데이터 수집 모듈
│   ├── processor/         # 데이터 처리 및 필터링 모듈
│   ├── summarizer/        # LLM 기반 요약 모듈
│   ├── publisher/         # 게시 모듈
│   ├── models/            # 데이터 모델
│   ├── api/               # API 라우트
│   ├── scheduler/         # 작업 스케줄러
│   └── templates/         # 웹 UI 템플릿
├── static/                # 정적 파일 (CSS, JS)
├── data/                  # 임시 데이터 저장소
├── tests/                 # 테스트 코드
├── requirements.txt       # 의존성 목록
└── README.md              # 프로젝트 문서
```

## 7. 주요 도전 과제 및 해결 방안

### 도전 과제
1. **웹 크롤링 안정성**: 뉴스 사이트 구조 변경 시 크롤러 실패
   - 해결: 유지보수가 쉬운 모듈식 크롤러 설계, 견고한 예외 처리
  
2. **LLM API 비용 관리**: OpenAI API 사용량과 비용 관리
   - 해결: 토큰 사용량 최적화, 캐싱 전략, 배치 처리

3. **중복 콘텐츠 처리**: 다양한 소스에서 발생하는 유사 콘텐츠 식별
   - 해결: 다층적 중복 감지 전략, 제목/내용 유사도 검사

4. **성능 확장성**: 1인 개발자가 유지보수 가능한 범위 내 확장성 확보
   - 해결: 모듈식 설계, 자동화된 테스트, 명확한 문서화

## 8. 개발 일정 요약

### 목업 개발 (1-2주)
- **1주차**: 데이터 수집 → 필터링 → LLM 요약 파이프라인 구축
- **2주차**: 웹 UI 개발 → 사용자 검토 기능 → 게시 모듈 구현

### 전체 시스템 개발 (3-4주)
- **3주차**: 데이터 소스 확장 → 고급 필터링 → DB 구현
- **4주차**: 성능 최적화 → UI/UX 개선 → 테스트 및 배포 준비

## 9. 개발 시작 체크리스트

- [ ] 기술 스택 최종 선택 및 개발 환경 설정
- [ ] 주요 RSS 피드 소스 목록 작성
- [ ] AI 관련 키워드 리스트 준비
- [ ] OpenAI API 키 발급 및 설정
- [ ] 네이버 개발자 센터 앱 등록 및 API 키 발급
- [ ] GitHub 저장소 생성
- [ ] 프로젝트 기본 구조 설정 

## 10. OpenAI Agent 및 MCP 서버 구현 계획

### OpenAI Agent 기반 아키텍처

OpenAI Agent를 활용하여, 시스템의 각 기능을 지능적인 에이전트로 구현합니다. 이를 통해 사람의 개입을 최소화하면서도 고품질의 콘텐츠 생성 및 관리가 가능해집니다.

#### 에이전트 구성

1. **수집 에이전트 (Collector Agent)**
   - 기능: RSS 피드, 웹 크롤링, 뉴스 API를 통한 콘텐츠 수집
   - 역할: 다양한 소스에서 AI 관련 뉴스를 효율적으로 수집하고 초기 데이터 정제
   - 의사결정: 데이터 소스의 신뢰성 평가, 수집 빈도 최적화, 오류 복구 전략 자동화

2. **선별 에이전트 (Curator Agent)**
   - 기능: 수집된 뉴스의 관련성 및 중요도 평가
   - 역할: 핵심 키워드 인식, 중복 콘텐츠 필터링, 콘텐츠 우선순위 지정
   - 의사결정: 트렌드 패턴 인식, 출처 신뢰도 평가, 사용자 관심사 학습 기반 선별

3. **요약 에이전트 (Summarizer Agent)**
   - 기능: 선별된 뉴스의 핵심 내용 추출 및 요약
   - 역할: 기사의 핵심 주장, 데이터, 의의를 파악하여 간결한 요약 생성
   - 의사결정: 요약 길이/형식 최적화, 핵심 정보 우선순위 판단, 다양한 관점 고려

4. **편집 에이전트 (Editor Agent)**
   - 기능: 생성된 요약의 품질 검토 및 개선
   - 역할: 문법/스타일 교정, 사실 확인, 일관성 유지
   - 의사결정: 수정 필요 여부 판단, 개선 방향 제안, 최종 품질 평가

5. **게시 에이전트 (Publisher Agent)**
   - 기능: 최종 콘텐츠의 게시 관리
   - 역할: 네이버 카페 등 게시 플랫폼 연동, 메타데이터 최적화
   - 의사결정: 최적 게시 시간 선정, 태그/카테고리 추천, 성과 데이터 수집 및 분석

#### 에이전트 간 협업 흐름

```
수집 에이전트 → 선별 에이전트 → 요약 에이전트 → 편집 에이전트 → 게시 에이전트
      ↑                 ↑                 ↑                ↑                ↑
      └─────────────────┴─────────────────┴────────────────┴────────────────┘
                              피드백 및 개선 루프
```

### MCP 서버 아키텍처

각 기능 모듈을 Model Context Protocol(MCP) 서버로 구현하여, 표준화된 인터페이스를 통해 LLM과 상호작용할 수 있도록 합니다. 이 아키텍처는 모듈 간 독립성과 재사용성을 높이고, 시스템 확장을 용이하게 합니다.

#### MCP 서버 모듈 구성

1. **RSS-MCP 서버**
   - 기능: RSS 피드 검색, 구독, 파싱을 위한 도구 제공
   - 인터페이스: `search_feeds()`, `subscribe_feed()`, `get_recent_articles()`
   - 구현: Python `feedparser` 라이브러리 기반, MCP 서버 래퍼

2. **웹 크롤링 MCP 서버**
   - 기능: 웹 페이지 크롤링, 콘텐츠 추출을 위한 도구 제공
   - 인터페이스: `crawl_page()`, `extract_content()`, `follow_links()`
   - 구현: `BeautifulSoup`/`Scrapy` 기반, 명시적 예외 처리 및 실패 복구 메커니즘

3. **콘텐츠 선별 MCP 서버**
   - 기능: 키워드 필터링, 중복 감지, 관련성 평가 도구 제공
   - 인터페이스: `filter_by_keywords()`, `detect_duplicates()`, `rank_relevance()`
   - 구현: 임베딩 기반 유사도 계산, 통계적 랭킹 알고리즘

4. **요약 생성 MCP 서버**
   - 기능: LLM 기반 요약 및 분석 생성 도구 제공
   - 인터페이스: `summarize_article()`, `analyze_impact()`, `generate_tags()`
   - 구현: OpenAI GPT API 또는 GEMMA3 27B API 활용, 프롬프트 최적화

5. **콘텐츠 게시 MCP 서버**
   - 기능: 네이버 카페 등 플랫폼 게시 도구 제공
   - 인터페이스: `publish_to_cafe()`, `schedule_post()`, `get_post_stats()`
   - 구현: 네이버 API 연동, OAuth 인증 처리, 오류 복구 메커니즘

#### MCP 서버 구현 로드맵

1. **기본 MCP 서버 템플릿 구현** (2주차 시작)
   - MCP 프로토콜 준수 기본 서버 구조 설계
   - 도구 등록, 실행, 오류 처리 표준화
   - 개발 문서 작성

2. **모듈별 MCP 서버 구현** (2-3주차)
   - 각 기능 모듈을 MCP 서버로 래핑
   - 인터페이스 정의 및 구현
   - 단위 테스트 구현

3. **OpenAI Agent와 MCP 서버 통합** (3주차 중반)
   - Agent와 MCP 서버 간 통신 구현
   - 도구 호출 및 응답 처리 최적화
   - 에러 핸들링 및 복구 메커니즘 구현

4. **시스템 통합 및 최적화** (4주차)
   - 모든 Agent와 MCP 서버 연동 테스트
   - 성능 최적화 및 병목 해결
   - 배포 준비 및 문서화

## 11. OpenAI Agent 및 MCP 서버 구현을 위한 기술 스택

### OpenAI Agent 구현
- **OpenAI Agents SDK**: Agent 정의 및 행동 프로그래밍
- **Python**: 기본 프로그래밍 언어
- **LangChain / LlamaIndex**: Agent 행동 체인 구현 보조
- **OpenAI Assistants API**: 고급 에이전트 기능 활용

### MCP 서버 구현
- **MCP Python SDK**: MCP 서버 구현을 위한 기본 프레임워크
- **FastAPI**: 기본 웹 서버 프레임워크
- **Uvicorn**: ASGI 서버
- **SSE (Server-Sent Events)**: 클라이언트-서버 실시간 통신

### 통합 환경
- **Docker**: 각 MCP 서버 및 Agent 컨테이너화
- **Docker Compose**: 다중 컨테이너 오케스트레이션
- **SQLite/mariadb**: 데이터 저장소
- **Redis**: 캐싱 및 메시지 큐

## 12. 수정된 개발 일정

### 목업 개발 (1-2주)
- **1주차**: 데이터 수집 → 필터링 → LLM 요약 파이프라인 구축
- **2주차**: 
  - 웹 UI 개발 → 사용자 검토 기능 → 게시 모듈 구현
  - **추가**: 기본 MCP 서버 템플릿 설계 및 프로토타입 구현

### 전체 시스템 개발 (3-4주)
- **3주차**: 
  - 데이터 소스 확장 → 고급 필터링 → DB 구현
  - **추가**: 모듈별 MCP 서버 구현, 초기 OpenAI Agent 통합
- **4주차**: 
  - 성능 최적화 → UI/UX 개선 → 테스트 및 배포 준비
  - **추가**: OpenAI Agent 및 MCP 서버 통합 완성, 시스템 안정화

## 13. 개발 시작 체크리스트 (수정됨)

- [ ] 기술 스택 최종 선택 및 개발 환경 설정
- [ ] 주요 RSS 피드 소스 목록 작성
- [ ] AI 관련 키워드 리스트 준비
- [ ] OpenAI API 키 발급 및 설정
- [ ] 네이버 개발자 센터 앱 등록 및 API 키 발급
- [ ] GitHub 저장소 생성
- [ ] 프로젝트 기본 구조 설정
- [ ] **OpenAI Agents SDK 및 API 접근 권한 확보**
- [ ] **MCP Python SDK 설치 및 기본 예제 테스트**
- [ ] **MCP 서버 템플릿 설계** 